{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness, Accountability, and Ethics, Spring 2024\n",
    "\n",
    "## Mandatory Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Models and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mapping for all features and their descriptions\n",
    "FEATURES = {\n",
    "   \"SCHL\":{\n",
    "      \"Description\":\"Educational attainment\",\n",
    "      \"Cat\":{\n",
    "         \"bb\":\"N/A (less than 3 years old)\",\n",
    "         \"1\":\"No schooling completed\",\n",
    "         \"2\":\"Nursery school, preschool\",\n",
    "         \"3\":\"Kindergarten\",\n",
    "         \"4\":\"Grade 1\",\n",
    "         \"5\":\"Grade 2\",\n",
    "         \"6\":\"Grade 3\",\n",
    "         \"7\":\"Grade 4\",\n",
    "         \"8\":\"Grade 5\",\n",
    "         \"9\":\"Grade 6\",\n",
    "         \"10\":\"Grade 7\",\n",
    "         \"11\":\"Grade 8\",\n",
    "         \"12\":\"Grade 9\",\n",
    "         \"13\":\"Grade 10\",\n",
    "         \"14\":\"Grade 11\",\n",
    "         \"15\":\"12th grade - no diploma\",\n",
    "         \"16\":\"Regular high school diploma\",\n",
    "         \"17\":\"GED or alternative credential\",\n",
    "         \"18\":\"Some college, but less than 1 year\",\n",
    "         \"19\":\"1 or more years of college credit, no degree\",\n",
    "         \"20\":\"Associate's degree\",\n",
    "         \"21\":\"Bachelor's degree\",\n",
    "         \"22\":\"Master's degree\",\n",
    "         \"23\":\"Professional degree beyond a bachelor's degree\",\n",
    "         \"24\":\"Doctorate degree\"\n",
    "      }\n",
    "   },\n",
    "   \"CIT\":{\n",
    "      \"Description\":\"Citizenship status\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"Born in the U.S.\",\n",
    "         \"2\":\"Born in Puerto Rico, Guam, the U.S. Virgin Islands, or the Northern Marianas\",\n",
    "         \"3\":\"Born abroad of American parent(s)\",\n",
    "         \"4\":\"U.S. citizen by naturalization\",\n",
    "         \"5\":\"Not a citizen of the U.S.\"\n",
    "      }\n",
    "   },\n",
    "   \"COW\":{\n",
    "      \"Description\":\"Class of worker\",\n",
    "      \"Cat\":{\n",
    "         \"b\":\"N/A (less than 16 years old/NILF who last worked more than 5 years ago or never worked)\",\n",
    "         \"1\":\"Employee of a private for-profit company or business, or of an individual, for wages, salary, or commissions\",\n",
    "         \"2\":\"Employee of a private not-for-profit, tax-exempt, or charitable organization\",\n",
    "         \"3\":\"Local government employee (city, county, etc.)\",\n",
    "         \"4\":\"State government employee\",\n",
    "         \"5\":\"Federal government employee\",\n",
    "         \"6\":\"Self-employed in own not incorporated business, professional practice, or farm\",\n",
    "         \"7\":\"Self-employed in own incorporated business, professional practice or farm\",\n",
    "         \"8\":\"Working without pay in family business or farm\",\n",
    "         \"9\":\"Unemployed and last worked 5 years ago or earlier or never worked\"\n",
    "      }\n",
    "   },\n",
    "   \"ENG\":{\n",
    "      \"Description\":\"Ability to speak English\",\n",
    "      \"Cat\":{\n",
    "         \"b\":\"N/A (less than 5 years old/speaks only English)\",\n",
    "         \"1\":\"Very well\",\n",
    "         \"2\":\"Well\",\n",
    "         \"3\":\"Not well\",\n",
    "         \"4\":\"Not at all\"\n",
    "      }\n",
    "   },\n",
    "   \"MAR\":{\n",
    "      \"Description\":\"Marital status\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"Married\",\n",
    "         \"2\":\"Widowed\",\n",
    "         \"3\":\"Divorced\",\n",
    "         \"4\":\"Separated\",\n",
    "         \"5\":\"Never married or under 15 years old\"\n",
    "      }\n",
    "   },\n",
    "   \"HINS1\":{\n",
    "      \"Description\":\"Insurance through a current or former employer or union\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"Yes\",\n",
    "         \"2\":\"No\"\n",
    "      }\n",
    "   },\n",
    "   \"HINS2\":{\n",
    "      \"Description\":\"Insurance purchased directly from an insurance company\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"Yes\",\n",
    "         \"2\":\"No\"\n",
    "      }\n",
    "   },\n",
    "   \"HINS4\":{\n",
    "      \"Description\":\"Medicaid, Medical Assistance, or any kind of government-assistance plan for those with low incomes or a disability\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"Yes\",\n",
    "         \"2\":\"No\"\n",
    "      }\n",
    "   },\n",
    "   \"RAC1P\":{\n",
    "      \"Description\":\"Recoded detailed race code\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"White alone\",\n",
    "         \"2\":\"Black or African American alone\",\n",
    "         \"3\":\"American Indian alone\",\n",
    "         \"4\":\"Alaska Native alone\",\n",
    "         \"5\":\"American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other races\",\n",
    "         \"6\":\"Asian alone\",\n",
    "         \"7\":\"Native Hawaiian and Other Pacific Islander alone\",\n",
    "         \"8\":\"Some Other Race alone\",\n",
    "         \"9\":\"Two or More Races\"\n",
    "      }\n",
    "   },\n",
    "   \"SEX\":{\n",
    "      \"Description\":\"Sex\",\n",
    "      \"Cat\":{\n",
    "         \"1\":\"Male\",\n",
    "         \"2\":\"Female\"\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "class FairnessReport:\n",
    "   '''\n",
    "   Custom class to compute a 'Fairness Report'. The Fairness report includes a measure of Statistical Parity, Equalized Odds,\n",
    "   and Equalized Outcomes from an array of TRUE labels, an array of PREDICTIONS, and the class split. If the probabilities are also \n",
    "   passed to the fitter, those are included in the results table.\n",
    "   '''\n",
    "\n",
    "   def __init__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "   def fit(self, y_true, y_pred, group, pred_prob=None, index=None):\n",
    "      '''\n",
    "      Fitter of the class. It basically constructs the table needed for computing the different fairness metrics.\n",
    "\n",
    "      Arguments:\n",
    "      - y_true: Array of TRUE labels\n",
    "      - y_pred: Array of PREDICTED labels\n",
    "      - group: Partititions for the observations\n",
    "      - pred_prob: The probabilities from the classifier.\n",
    "      - index: Index of the observations. If passed, the index from the observations is kept, so easy comparisons can be made.\n",
    "      '''\n",
    "\n",
    "      # Create a DF with the each prediction, including group, prediction (selected) and target (true label)\n",
    "      self.results_table = pd.DataFrame()\n",
    "      self.results_table[\"group\"] = group\n",
    "      self.results_table[\"target\"] = y_true\n",
    "      self.results_table[\"selected\"] = y_pred\n",
    "      if type(pred_prob) != None:\n",
    "         self.results_table[\"pred_prob\"] = pred_prob[:,1]\n",
    "      if type(index) != None:\n",
    "         try:\n",
    "            self.results_table.index = index\n",
    "         except:\n",
    "            print(\"Could not include index. Omitted.\")\n",
    "      \n",
    "      return\n",
    "      \n",
    "\n",
    "   def compute(self):\n",
    "      '''\n",
    "      Compute the Fairness metrics, and show group-wise Confusion Matrices along with computed Statistical Parity, Equalized Odds and Equalized Outcomes.\n",
    "      '''\n",
    "\n",
    "      # Extract TPR and FPR per group\n",
    "      rates = {1: {}, 2: {}}\n",
    "\n",
    "      # Colormaps\n",
    "      colors = [plt.cm.Blues, plt.cm.Reds]\n",
    "\n",
    "      # Plot confusion matrices for context\n",
    "      fig, ax = plt.subplots(2, 2, figsize=(12,12), dpi=300)\n",
    "\n",
    "      # Populate the subplots\n",
    "      for ix in range(2):\n",
    "         group = ix+1 # Group code (1: Male, 2: Female)\n",
    "         # Compute the confusion matrix\n",
    "         cm = confusion_matrix(self.results_table.loc[self.results_table.group == group, \"target\"], self.results_table.loc[self.results_table.group == group, \"selected\"],\n",
    "                                                      normalize=None)\n",
    "         # Compute the row-wise normalized matrix\n",
    "         cm_norm = confusion_matrix(self.results_table.loc[self.results_table.group == group, \"target\"], self.results_table.loc[self.results_table.group == group, \"selected\"],\n",
    "                                                      normalize=\"true\")\n",
    "         # Extract True Positive Rate (TPR) and False Positive Rate (FPR) from the Confusion Matrix\n",
    "         rates[group][\"TPR\"] = cm_norm[1,1]\n",
    "         rates[group][\"FPR\"] = cm_norm[0,1]\n",
    "\n",
    "         # Display both CMs\n",
    "         disp1 = ConfusionMatrixDisplay(cm)\n",
    "         disp2 = ConfusionMatrixDisplay(cm_norm)\n",
    "         disp1.plot(cmap=colors[ix], ax=ax[ix,0], values_format = 'g')\n",
    "         disp2.plot(cmap=colors[ix], ax=ax[ix,1])\n",
    "         ax[ix,0].set_title(f'Group {group}')\n",
    "         ax[ix,1].set_title(f'Group {group} - Row Norm.')\n",
    "         \n",
    "      ax[0,0].set_xlabel(\"\")\n",
    "      ax[0,1].set_xlabel(\"\")\n",
    "      ax[0,1].set_ylabel(\"\")\n",
    "      ax[1,1].set_ylabel(\"\")\n",
    "      fig.suptitle(\"Confusion Matrices per Group\", fontweight='bold')\n",
    "      plt.show();\n",
    "\n",
    "      # Statistical parity: Prob. of being selected given the group\n",
    "      ## Per each group, we look at the number of TRUE Predictions and divide by the total number of people in that group\n",
    "      G1 = len(self.results_table.loc[(self.results_table.group == 1) & (self.results_table.selected)]) / len(self.results_table.loc[(self.results_table.group == 1)])\n",
    "      G2 = len(self.results_table.loc[(self.results_table.group == 2) & (self.results_table.selected)]) / len(self.results_table.loc[(self.results_table.group == 2)])\n",
    "      print(f\"Statistical Parity: {G1 == G2} -> (P(s=1 | G=1): {G1:.3f} & P(s=1 | G=2): {G2:.3f})\")\n",
    "\n",
    "      # Equalized odds: both TPR and FPR are equal for both groups\n",
    "      print(f'Equalized Odds: {(rates[1][\"TPR\"] == rates[2][\"TPR\"]) & (rates[1][\"FPR\"] == rates[2][\"FPR\"])} -> TPR Group 1: {rates[1][\"TPR\"]:.3f} & TPR Group 2: {rates[2][\"TPR\"]:.3f} & FPR Group 1: {rates[1][\"FPR\"]:.3f} & FPR Group 2: {rates[2][\"FPR\"]:.3f}')\n",
    "\n",
    "      # Equalized outcomes: Given the prediction, trues are independent of the group\n",
    "      ## Per each group:\n",
    "         ### we look at the True Positives (TP, i.e., the number of people that were predicted POSITIVE AND had POSITIVE TRUE label) and divide by the number of predicted TRUE in that group.\n",
    "         ### Then we look at the False Negatives (FN, i.e., the number of people that had POSITIVE LABEL but were NOT predicted POSITIVE)\n",
    "      G1_pred_pos = len(self.results_table.loc[(self.results_table.group == 1) & (self.results_table.target) & (self.results_table.selected)]) / len(self.results_table.loc[(self.results_table.group == 1) & (self.results_table.selected)])\n",
    "      G2_pred_pos = len(self.results_table.loc[(self.results_table.group == 2) & (self.results_table.target) & (self.results_table.selected)]) / len(self.results_table.loc[(self.results_table.group == 2) & (self.results_table.selected)])\n",
    "      G1_pred_neg = len(self.results_table.loc[(self.results_table.group == 1) & (self.results_table.target) & (~self.results_table.selected)]) / len(self.results_table.loc[(self.results_table.group == 1) & (~self.results_table.selected)])\n",
    "      G2_pred_neg = len(self.results_table.loc[(self.results_table.group == 2) & (self.results_table.target) & (~self.results_table.selected)]) / len(self.results_table.loc[(self.results_table.group == 2) & (~self.results_table.selected)])\n",
    "      print(f\"Equalized Outcomes: {(G1_pred_pos == G2_pred_pos) & (G1_pred_neg == G2_pred_neg)} -> P(T=1 | G=1,S=1): {G1_pred_pos:.3f} & P(T=1 | G=2, S=1): {G2_pred_pos:.3f} P(T=1 | G=1, S=0): {G1_pred_neg:.3f} & P(T=1 | G=2, S=0): {G2_pred_neg:.3f}\")\n",
    "\n",
    "      return\n",
    "\n",
    "\n",
    "   def get_results_table(self):\n",
    "      '''\n",
    "      Return the results table for inspection.\n",
    "      '''\n",
    "      return self.results_table\n",
    "\n",
    "\n",
    "   def plot_roc_curves(self, highlight={}):\n",
    "      '''\n",
    "      Plot the group-wise ROC curves.\n",
    "      '''\n",
    "\n",
    "      if \"pred_prob\" not in self.results_table.columns:\n",
    "         raise Exception(\"The predicted probabilities were not passed to the fitter.\")\n",
    "      \n",
    "      # Plot ROC curve\n",
    "      plt.figure(figsize=(8,8), dpi=300)\n",
    "\n",
    "      # Color list\n",
    "      colormap = [\"darkblue\", \"darkred\", \"darkmagenta\", \"darkcyan\", \"darksalmon\", \"darkkhari\"]\n",
    "\n",
    "      # Predict probabilities for the test set, group-wise\n",
    "      for ix, group in enumerate(self.results_table[\"group\"].unique()):\n",
    "         group_subset = self.results_table[self.results_table[\"group\"] == group]\n",
    "         # Compute ROC curve and ROC area for each group\n",
    "         fpr, tpr, th = roc_curve(group_subset.target, group_subset.pred_prob)\n",
    "         roc_auc = roc_auc_score(group_subset.target, group_subset.pred_prob)\n",
    "         # Plot the ROC curve\n",
    "         plt.plot(fpr, tpr, color=colormap[ix], lw=2, label=f'ROC curve (Group {group})\\n(area = {roc_auc:.2f})')\n",
    "         # If a threshold should be highlighted\n",
    "         if group in highlight:\n",
    "            # Get the threshold point\n",
    "            t = highlight[group]\n",
    "            # Find the closest threshold to the requested point\n",
    "            idx = (np.abs(th - t)).argmin()\n",
    "            plt.plot(fpr[idx], tpr[idx], marker='X', markersize=10, color=colormap[ix])\n",
    "            plt.vlines(fpr[idx], 0, tpr[idx], colors=colormap[ix], linestyles='dashed', label='', alpha=0.5)\n",
    "            plt.text(fpr[idx], tpr[idx]/2, f\"{fpr[idx]:.2f}\", color=\"gray\", rotation=\"vertical\")\n",
    "            plt.hlines(tpr[idx], 0, fpr[idx], colors=colormap[ix], linestyles='dashed', label='', alpha=0.5)\n",
    "            plt.text(fpr[idx]/2, tpr[idx], f\"{tpr[idx]:.2f}\", color=\"gray\", rotation=\"horizontal\")\n",
    "\n",
    "\n",
    "      # Plot the straight line\n",
    "      plt.plot([0, 1], [0, 1], color='green', lw=1, linestyle='--')\n",
    "      \n",
    "      # Format the plot\n",
    "      plt.xlim([0.0, 1.0])\n",
    "      plt.ylim([0.0, 1.0])\n",
    "      plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "      plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "      plt.title('Receiver Operating Characteristic (ROC)', fontweight='bold', fontsize=16)\n",
    "      plt.legend(loc=\"lower right\")\n",
    "      plt.show();\n",
    "\n",
    "      return\n",
    "         \n",
    "\n",
    "class ExplainabilityReport:\n",
    "   '''\n",
    "   Custom class to provide explainability from feature importance in a model.\n",
    "\n",
    "   Arguments:\n",
    "    - feature_coef: Array with the coefficients for the features in the model\n",
    "    - feature_names: Array with the coded names for the features, in the same order as passed in feature_coef.\n",
    "    - feat_description: Map with human-readable description for the coded features.\n",
    "    - mode_name: Model name ID for plot title.\n",
    "   '''\n",
    "\n",
    "   def __init__(self, feature_coef, feature_names, feat_description=None, model_name=\"\"):\n",
    "\n",
    "      # For nice plot titles\n",
    "      self.model_name = model_name\n",
    "      # Calculate the absolute magnitude of coefficients\n",
    "      abs_coefficients = np.abs(feature_coef)\n",
    "      # Normalize the coefficients to get their relative importance\n",
    "      normalized_coefficients = abs_coefficients / np.sum(abs_coefficients)\n",
    "      self.feature_importance = pd.DataFrame()\n",
    "      self.feature_importance[\"Feature\"] = feature_names\n",
    "      self.feature_importance[\"Import. (%)\"] = normalized_coefficients.round(4) * 100\n",
    "      self.feature_importance[\"Coef.\"] = feature_coef\n",
    "\n",
    "      if feat_description:\n",
    "         # Add the description for easy context\n",
    "         descriptions = []\n",
    "         for ix, row in self.feature_importance.iterrows():\n",
    "               try:\n",
    "                  k, v = row[\"Feature\"].split(\"_\")\n",
    "                  desc = feat_description[k][\"Description\"] + \": \" + feat_description[k][\"Cat\"][v]\n",
    "               except:\n",
    "                  desc = None\n",
    "               descriptions.append(desc)\n",
    "\n",
    "         self.feature_importance[\"Desc.\"] = descriptions\n",
    "      \n",
    "      # Order table by feature importance\n",
    "      self.feature_importance = self.feature_importance.sort_values(\"Import. (%)\", ascending=False)\n",
    "      \n",
    "      return\n",
    "\n",
    "\n",
    "   def get_feature_importance(self):\n",
    "      # Return the feature importance table\n",
    "      return self.feature_importance.sort_values(\"Import. (%)\", ascending=False)\n",
    "\n",
    "\n",
    "   def plot_feature_importance(self):\n",
    "      # Create a diverging horizontal bar plotfg\n",
    "      fig, ax = plt.subplots(figsize=(12,12), dpi=300)\n",
    "\n",
    "      # Invert the order of the feature importance so the most important features come on top\n",
    "      feature_importance = self.feature_importance.sort_values(\"Import. (%)\", ascending=True)\n",
    "      feature_importance = feature_importance[feature_importance[\"Import. (%)\"] >= 1]\n",
    "      bar_colors = ['red' if score < 0 else 'blue' for score in feature_importance[\"Coef.\"]]\n",
    "\n",
    "      bars = ax.barh(feature_importance[\"Feature\"], feature_importance[\"Coef.\"], color=bar_colors)\n",
    "\n",
    "      # Add importance %\n",
    "      for bar, score, importance in zip(bars, feature_importance[\"Coef.\"], feature_importance[\"Import. (%)\"]):\n",
    "         if score < 0:\n",
    "            ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{importance/100:.1%}',\n",
    "                     va='center', ha='left', color='black', fontsize=10, fontweight='bold')\n",
    "         else:\n",
    "            ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{importance/100:.1%}',\n",
    "                     va='center', ha='right', color='white', fontsize=10, fontweight='bold')\n",
    "\n",
    "      # Add title and labels\n",
    "      ax.set_title(f'Feature Importance\\n{self.model_name}', fontweight=\"bold\", fontsize=16)\n",
    "      ax.set_xlabel('Importance Score', fontsize=12, fontweight=\"bold\")\n",
    "      ax.set_ylabel(\"Feature\", fontsize=12, fontweight=\"bold\")\n",
    "      ax.text(0.5, 0,\"Features with less than 1%\\nimportance are not displayed.\", horizontalalignment=\"center\", fontsize=16, color=\"white\", bbox=dict(facecolor='black', alpha=0.9))\n",
    "\n",
    "      # Add grid\n",
    "      ax.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "      # Show plot\n",
    "      plt.show();\n",
    "\n",
    "\n",
    "def report_metrics(y_true, y_pred, model_name=\"\"):\n",
    "   '''\n",
    "   Function assess a model's prediction main metrics (accuracy, F1-Score, Recall, Precision and Confusion Matrix)\n",
    "   '''\n",
    "\n",
    "   # Let's get some metrics for the model\n",
    "   print(classification_report(y_true, y_pred))\n",
    "\n",
    "   disp = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred, normalize=\"true\"))\n",
    "   disp.plot(cmap=plt.cm.Greens)\n",
    "   plt.title(f\"Confusion Matrix\\n{model_name}\", fontweight='bold', fontsize=16)\n",
    "   plt.show();\n",
    "\n",
    "def find_counterfactual(X_test, predic_results, feature_priority, feat_description=None):\n",
    "    '''\n",
    "    Custom function to find a conterfactual example by looking at observations with different prediction outcome and least possible differences in feature space.\n",
    "    '''\n",
    "\n",
    "    # Sort the observations by feature importance, so similar observations are contiguos\n",
    "    observations = X_test.sort_values(feature_priority)\n",
    "    observations = observations[feature_priority] # We re-arrange the columns from higher importance to lower\n",
    "    # Convert booleans to integers for easy euclidean calculation\n",
    "    observations = observations.astype(int)\n",
    "    # Merge the observations and features with the results from the model's prediction\n",
    "    merged = observations.merge(predic_results, how=\"left\", left_index=True, right_index=True)\n",
    "    ### We will use Euclidean distance as a measure of how similar two observations are\n",
    "    # Remove AGE from the euclidean distance calculation\n",
    "    euclideand_cols = feature_priority.copy()\n",
    "    euclideand_cols.remove(\"AGEP\")\n",
    "    # Extract indexes for similar observations with different predictions\n",
    "    ixs = None\n",
    "    min_diff = np.inf\n",
    "    # Loop over the observations to find counterfactuals\n",
    "    for i in range(len(merged)-1):\n",
    "        # Check if their prediction is different\n",
    "        if merged.iloc[i].selected != merged.iloc[i+1].selected:\n",
    "            # Check how different the two observations are\n",
    "            obsA = merged.loc[merged.index[i], euclideand_cols]\n",
    "            obsB = merged.loc[merged.index[i+1], euclideand_cols]\n",
    "            if merged.loc[merged.index[i], \"AGEP\"] != merged.loc[merged.index[i+1], \"AGEP\"]:\n",
    "                obsA[\"AGE\"] = 1\n",
    "                obsB[\"AGE\"] = 0\n",
    "            euclidean_distance = np.linalg.norm(obsA - obsB)\n",
    "            \n",
    "            if euclidean_distance < min_diff:\n",
    "                ixs = [i, i+1]\n",
    "                min_diff = euclidean_distance\n",
    "\n",
    "    # We transpose the results for easier visibility\n",
    "    results = merged.iloc[ixs].T\n",
    "    # If the feature description is available, we use it for better context\n",
    "    descriptions = []\n",
    "    if feat_description:\n",
    "        # For each feature\n",
    "        for ix, row in results.iterrows():\n",
    "            # If the observations differ\n",
    "            if row.iloc[0] != row.iloc[1]:\n",
    "                try:\n",
    "                    k, v = ix.split(\"_\")\n",
    "                    desc = feat_description[k][\"Description\"] + \": \" + feat_description[k][\"Cat\"][v]\n",
    "                except:\n",
    "                    desc = \"N/A\"\n",
    "            else:\n",
    "              desc = \"\"\n",
    "\n",
    "            descriptions.append(desc)\n",
    "    \n",
    "        results[\"Desc.\"] = descriptions\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset load and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    df[\"SCHL\"] = df[\"SCHL\"].astype(int)\n",
    "    df[\"COW\"] = df[\"COW\"].astype(int)\n",
    "    df[\"ENG\"] = df[\"ENG\"].fillna(0).astype(int)\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = [ \"HINS1\", \"HINS2\", \"HINS4\", \"CIT\", \"COW\", \"SCHL\", \"MAR\", \"SEX\", \"RAC1P\"]\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=False, columns=cols, drop_first=True)\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"ENG\"], drop_first=True)\n",
    "    return df_processed, df, target, sex\n",
    "\n",
    "data, data_original, target, group = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0) ## SHOULD WE CHANGE THE SEED?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (Classifiers and fairness considerations)\n",
    "## 1.1 White-box model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature processing\n",
    "## Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(solver=\"newton-cholesky\")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the results\n",
    "y_hat = lr.predict(X_test_scaled)\n",
    "pred_probs = lr.predict_proba(X_test_scaled)\n",
    "\n",
    "# Get the score metrics and Confusion Matrix\n",
    "report_metrics(y_test, y_hat, model_name = \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fairness assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our FairnessReport class to look at the group-wise results and check for fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FairnessReport()\n",
    "fr.fit(y_true = y_test, y_pred = y_hat, group = group_test, pred_prob = pred_probs, index=X_test.index)\n",
    "fr.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 ROC Curves per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.plot_roc_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Ensuring Statistical Parity a posteriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model predicts TRUE with different rates, we will ensure statistical parity by turning those predictions closer to the threshold to TRUE, for the minority group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the table of results, along with the probabilities returned by our Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve the table of results\n",
    "results = fr.get_results_table()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the results per group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the results summmary, to look at the disparity\n",
    "results_per_group = results[[\"group\", \"selected\"]].groupby(\"group\").sum(\"selected\")\n",
    "results_per_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_group = results_per_group.idxmin().values[0]\n",
    "majority_group = results_per_group.idxmax().values[0]\n",
    "diff = results_per_group.selected.max() - results_per_group.selected.min()\n",
    "\n",
    "print(f\"The minority group is {minority_group}, with {diff} less observations predicted as TRUE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Reduce the number of TRUE predictions for the majority group to match the minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = results[(results.group == majority_group) & (results.selected == True)].sort_values(\"pred_prob\", ascending=True)\n",
    "ix = sorted_results[:diff].index\n",
    "\n",
    "results_method1 = results.copy()\n",
    "\n",
    "results_method1.loc[ix, \"selected\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the results summmary, to look at the disparity\n",
    "results_per_group_m1 = results_method1[[\"group\", \"selected\"]].groupby(\"group\").sum(\"selected\")\n",
    "results_per_group_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some metrics for the model\n",
    "report_metrics(y_true=results_method1.target, y_pred=results_method1.selected, model_name=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that with method 1, the accuracy dropped only 2 percent, while the TPR decreased from 0.82 to 0.71, meaning that we lost some TRUE positives from the switching labels on the majority group. Nevertheless, the FPR also decreased from 0.29 to 0.22, meaning that many of the flipped labels are were incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Increase the number of positive predictions for the minority group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now sort the results for the minotirity group observations that were not predicted TRUE, and pick the closest ones to the threshold, and flip their prediction to achieve statistical parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = results[(results.group == minority_group) & (results.selected == False)].sort_values(\"pred_prob\", ascending=False)\n",
    "ix = sorted_results.iloc[:diff].index\n",
    "\n",
    "new_th = sorted_results.loc[ix[-1], \"pred_prob\"]\n",
    "\n",
    "results_method2 = results.copy()\n",
    "\n",
    "results_method2.loc[ix, \"selected\"] = True\n",
    "\n",
    "print(f\"We flip {diff} predictions, reducing the threhsold for group {minority_group} from 0.5 to {new_th:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recalculate the number of predictions as TRUE per group, and re compute the main metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can look at the results summmary, to look at the disparity\n",
    "results_per_group = results_method2[[\"group\", \"selected\"]].groupby(\"group\").sum(\"selected\")\n",
    "results_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some metrics for the model\n",
    "report_metrics(results_method2.target, results_method2.selected, model_name=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that accuracy dropped only 2 percent, while the TPR increased from 0.82 to 0.89, meaning that we were able to capture more TRUE positives from the switching labels on the minority group. Nevertheless, the FPR also increased from 0.29 to 0.42, meaning that many of the flipped labels are now INCORRECT predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look the ROC curve with the separated threhsolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the group-wise ROC curves, highlighting the thresholds used for prediction for each group. \n",
    "fr.plot_roc_curves(highlight={1: 0.50, 2: new_th})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Black-box model: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight=\"balanced_subsample\", n_jobs=-1, verbose=0, random_state=seed)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the results\n",
    "y_hat = rf.predict(X_test)\n",
    "pred_probs = rf.predict_proba(X_test)\n",
    "\n",
    "# Let's get some metrics for the model\n",
    "report_metrics(y_test, y_hat, model_name=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Fairness assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FairnessReport()\n",
    "fr.fit(y_true = y_test, y_pred = y_hat, group = group_test, pred_prob = pred_probs, index=X_test.index)\n",
    "fr.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (Explaining white-box models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients\n",
    "coefficients = lr.coef_[0]\n",
    "coef_names = X_train.columns\n",
    "\n",
    "er = ExplainabilityReport(coefficients, coef_names, FEATURES, model_name=\"Logistic Regression\")\n",
    "er.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er.plot_feature_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that accuracy dropped only 2 percent, while the TPR increased from 0.82 to 0.89, meaning that we were able to capture more TRUE positives from the switching labels on the minority group. Nevertheless, the FPR also increased from 0.29 to 0.42, meaning that many of the flipped labels are now INCORRECT predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a counterfactual example, we will look at any observation that was predicted TRUE, but with probability close to the threshold, under the assumption that a small change in any of its features could have put the prediction probability under the threshold and thus change the prediction. Then, we will find from the observations predicted as FALSE, the closest to that one found as TRUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the least of features ordered by importance\n",
    "cols = list(er.get_feature_importance().Feature)\n",
    "counterfactuals = find_counterfactual(X_test, results, feature_priority=cols, feat_description=FEATURES)\n",
    "counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that observation 374998 (predicted positive) and observation 140871 (predicted negative) only differ in that 374998 has bought purchased insurance directly from an insurance company, while observation 140871 hasn't. That difference alone makes the model predict TRUE for observation 374998 (meaning he will make more than 35.000 USD) and observation 140871 FALSE (not making 35.000 USD)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
